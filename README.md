# Fabric-Hackathon
Hand Sign to Text Generator

## Introduction
In a world where communication is key, we strive to break barriers for the deaf community. Our Sign Language Translator is not just a technology showcase, but a beacon of inclusivity and accessibility. By harnessing the power of cutting-edge AI and machine learning, we aim to bridge the gap between spoken language and sign language, empowering deaf and dumb individuals to fully participate in conversations, meetings, and everyday interactions.

Join us on this journey as we revolutionize communication for all.Utilizing the robust capabilities of Microsoft Fabric, our Sign Language Translator takes a significant step forward in enhancing accessibility for the deaf and dumb community. Microsoft's extensive collaboration tools seamlessly integrate with our solution, propelling our vision into reality. By leveraging Microsoft's compatibility and expansive network, we can extend our reach to countless individuals within the deaf and dumb community, offering them a lifeline to effective communication. Together with Microsoft, we embark on a journey to empower and enrich the lives of those who have long been underserved in the realm of communication.

## About
Our project embarks on a multifaceted journey aimed at revolutionizing communication accessibility for the deaf and hard-of-hearing communities. At its core lies a sophisticated process that begins with the meticulous capture of sign language gestures through video recordings. These videos serve as the raw material from which we derive our dataset, each frame encapsulating a crucial moment in the expression of language through hand gestures.

To manage and manipulate this vast trove of visual data efficiently, we implement pickling techniques, a method of serialization that ensures the integrity and accessibility of our dataset throughout its lifecycle. This allows us to streamline the processing pipeline, facilitating smooth transitions from video to individual image frames, laying the groundwork for subsequent analysis and interpretation.

Our dataset, now transformed into a structured format suitable for machine learning, undergoes intensive training utilizing the Random Forest classifier. This algorithm, renowned for its versatility and robustness, navigates the complexities inherent in sign language recognition, learning to discern patterns and nuances within the visual cues presented.

As the model refines its understanding through iterative training cycles, it evolves into a proficient interpreter of sign language gestures, capable of accurately predicting the corresponding alphabet characters with increasing precision. These predictions are not isolated entities but rather components of a larger linguistic context, forming the building blocks of coherent words and phrases.

Yet, our vision extends beyond mere recognition and interpretation. By integrating the power of Azure and OpenAI, we augment our solution with an innovative Auto-Correct mechanism, infused with the intelligence of artificial intelligence. This transformative feature analyses the concatenated outputs generated by our model, applying context-aware corrections and refinements in real-time.

In essence, our project represents a convergence of cutting-edge technology and compassionate innovation, empowering individuals within the deaf and hard-of-hearing communities to engage in seamless, meaningful communication. Through our endeavours, we aspire to break down barriers, foster inclusivity, and pave the way towards a world where communication knows no bounds.

## Data Collection
![Capture](https://github.com/Cycl0tr0n/Fabric-Hackathon/assets/129075535/15c48623-a0cd-4260-a1d7-8cb47046972b)

## Dataset Creation
```python
import os
import pickle

import mediapipe as mp
import cv2
import matplotlib.pyplot as plt


mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)

DATA_DIR = './data'

data = []
labels = []
for dir_ in os.listdir(DATA_DIR):
    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):
        data_aux = []

        x_ = []
        y_ = []

        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        results = hands.process(img_rgb)
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                for i in range(len(hand_landmarks.landmark)):
                    x = hand_landmarks.landmark[i].x
                    y = hand_landmarks.landmark[i].y

                    x_.append(x)
                    y_.append(y)

                for i in range(len(hand_landmarks.landmark)):
                    x = hand_landmarks.landmark[i].x
                    y = hand_landmarks.landmark[i].y
                    data_aux.append(x - min(x_))
                    data_aux.append(y - min(y_))

            data.append(data_aux)
            labels.append(dir_)

f = open('data.pickle', 'wb')
pickle.dump({'data': data, 'labels': labels}, f)
f.close()
```
