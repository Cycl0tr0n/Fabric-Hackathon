# Fabric-Hackathon
Hand Sign to Text Generator

## Introduction
In a world where communication is key, we strive to break barriers for the deaf community. Our Sign Language Translator is not just a technology showcase, but a beacon of inclusivity and accessibility. By harnessing the power of cutting-edge AI and machine learning, we aim to bridge the gap between spoken language and sign language, empowering deaf and dumb individuals to fully participate in conversations, meetings, and everyday interactions.

Join us on this journey as we revolutionize communication for all.Utilizing the robust capabilities of Microsoft Fabric, our Sign Language Translator takes a significant step forward in enhancing accessibility for the deaf and dumb community. Microsoft's extensive collaboration tools seamlessly integrate with our solution, propelling our vision into reality. By leveraging Microsoft's compatibility and expansive network, we can extend our reach to countless individuals within the deaf and dumb community, offering them a lifeline to effective communication. Together with Microsoft, we embark on a journey to empower and enrich the lives of those who have long been underserved in the realm of communication.

## About
Our project embarks on a multifaceted journey aimed at revolutionizing communication accessibility for the deaf and hard-of-hearing communities. At its core lies a sophisticated process that begins with the meticulous capture of sign language gestures through video recordings. These videos serve as the raw material from which we derive our dataset, each frame encapsulating a crucial moment in the expression of language through hand gestures.

To manage and manipulate this vast trove of visual data efficiently, we implement pickling techniques, a method of serialization that ensures the integrity and accessibility of our dataset throughout its lifecycle. This allows us to streamline the processing pipeline, facilitating smooth transitions from video to individual image frames, laying the groundwork for subsequent analysis and interpretation.

Our dataset, now transformed into a structured format suitable for machine learning, undergoes intensive training utilizing the Random Forest classifier. This algorithm, renowned for its versatility and robustness, navigates the complexities inherent in sign language recognition, learning to discern patterns and nuances within the visual cues presented.

As the model refines its understanding through iterative training cycles, it evolves into a proficient interpreter of sign language gestures, capable of accurately predicting the corresponding alphabet characters with increasing precision. These predictions are not isolated entities but rather components of a larger linguistic context, forming the building blocks of coherent words and phrases.

Yet, our vision extends beyond mere recognition and interpretation. By integrating the power of Azure and OpenAI, we augment our solution with an innovative Auto-Correct mechanism, infused with the intelligence of artificial intelligence. This transformative feature analyses the concatenated outputs generated by our model, applying context-aware corrections and refinements in real-time.

In essence, our project represents a convergence of cutting-edge technology and compassionate innovation, empowering individuals within the deaf and hard-of-hearing communities to engage in seamless, meaningful communication. Through our endeavours, we aspire to break down barriers, foster inclusivity, and pave the way towards a world where communication knows no bounds.

## Data Collection
First we capture images from a webcam and organizes them into different classes. Where we create a directory for each class and saves a specified number of images for each class into their respective directories. The images are captured. This could be useful for creating a dataset for machine learning purposes.

## Dataset Creation
Then we recognise hand gesture. We read images from a directory, use the MediaPipe library to detect hand landmarks, normalizes these landmarks, and stores them along with their labels in a pickle file for later use. The landmarks are points on the hand, like the tips of the fingers and the wrist. Normalizing the landmarks involves adjusting their coordinates relative to the minimum x and y values. This is done to make the data uniform regardless of where the hand was in the image. The pickle file can be used later for machine learning task as training the model.

## Training the model
We train a machine learning model to recognize hand gestures. It loads hand gesture data and labels from a pickle file, splits the data into training and testing sets, trains a Random Forest Classifier on the training data, and evaluates its performance on the testing data. The trained model is then saved to a pickle file for future use.

## Model Prediction
This script captures video from the webcam, detects hand landmarks in each frame using MediaPipe, and uses a pre-trained model to predict the hand gesture being made. The predicted gesture is then displayed on the video frame. Then we process frames in real-time until manually stopped.

## Auto-Correction Using Azure Open-AI
Here we provide the original text and the code sends it to the model, which is trained on examples of good writing. The model identifies errors and sends back a corrected version. The code then compares the corrected text with the original. In essence, this code acts like a proofreader, helping to improve our text.
